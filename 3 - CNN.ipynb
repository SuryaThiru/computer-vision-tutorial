{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Convolution ? - brief\n",
    "\n",
    "In machine learning, a convolutional neural network is a class of deep, feed-forward artificial neural networks, most commonly applied to analyzing visual imagery. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What inspired Convolutional Networks?\n",
    "\n",
    "CNNs are biologically-inspired models inspired by research by D. H. Hubel and T. N. Wiesel. They proposed an explanation for the way in which mammals visually perceive the world around them using a layered architecture of neurons in the brain, and this in turn inspired engineers to attempt to develop similar pattern recognition mechanisms in computer vision.\n",
    "\n",
    "In their hypothesis, within the visual cortex, complex functional responses generated by \"complex cells\" are constructed from more simplistic responses from \"simple cells'.\n",
    "\n",
    "For instances, simple cells would respond to oriented edges etc, while complex cells will also respond to oriented edges but with a degree of spatial invariance.\n",
    "\n",
    "Receptive fields exist for cells, where a cell responds to a summation of inputs from other local cells.\n",
    "\n",
    "The architecture of deep convolutional neural networks was inspired by the ideas mentioned above\n",
    "\n",
    "- local connections\n",
    "- layering\n",
    "- spatial invariance (shifting the input signal results in an equally shifted output signal. , most of us are able to recognize specific faces under a variety of conditions because we learn abstraction These abstractions are thus invariant to size, contrast, rotation, orientation.\n",
    "\n",
    "However, it remains to be seen if these computational mechanisms of convolutional neural networks are similar to the computation mechanisms occurring in the primate visual system\n",
    "\n",
    "- convolution operation\n",
    "- shared weights\n",
    "- pooling/subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work? \n",
    "\n",
    "![CNN Layer wise structure](assets/cnn_layer1.jpg)\n",
    "<br>\n",
    "<bold>You can see how an image is being restructured in each layer to get useful features out of an image</bold>\n",
    "<br>\n",
    "![CNN what happens to a pixels window](assets/cnn_layer2.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Convolution Neural network\n",
    "\n",
    "###### We are gonna create a convolution neural network to classify whether there is cat or a dog in a given image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 - Prepare the dataset of Images\n",
    "\n",
    "What exactly would data contain\n",
    "\n",
    "![RGB channel](assets/rgb.webp)\n",
    "\n",
    "- Every image is a matrix of pixel values. \n",
    "- The range of values that can be encoded in each pixel depends upon its bit size. \n",
    "- Most commonly, we have 8 bit or 1 Byte-sized pixels. Thus the possible range of values a single pixel can represent is [0, 255]. \n",
    "- However, with coloured images, particularly RGB (Red, Green, Blue)-based images, the presence of separate colour channels (3 in the case of RGB images) introduces an additional ‘depth’ field to the data, making the input 3-dimensional. \n",
    "- Hence, for a given RGB image of size, say 255×255 (Width x Height) pixels, we’ll have 3 matrices associated with each image, one for each of the colour channels. \n",
    "- Thus the image in it’s entirety, constitutes a 3-dimensional structure called the Input Volume (255x255x3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "nrml = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x_train', 'y_train', 'x_test', 'y_test'])\n"
     ]
    }
   ],
   "source": [
    "# let us load the data\n",
    "import pickle\n",
    "\n",
    "with open('data/data.pickle', 'rb') as h:\n",
    "    pkl_data = pickle.load(h)\n",
    "print(pkl_data.keys())\n",
    "x_train = pkl_data['x_train']\n",
    "y_train = pkl_data['y_train']\n",
    "x_test = pkl_data['x_test']\n",
    "y_test = pkl_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape (18706, 64, 64, 3), test images shape(6240, 64, 64, 3)\n",
      "train labels shape (18706, 2), test labels shape(6240, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"train images shape {}, test images shape{}\".format(x_train.shape, x_test.shape))\n",
    "print(\"train labels shape {}, test labels shape{}\".format(y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Visualization\n",
    "Take a look at what kind of data you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets look at some of our pictures\n",
    "\n",
    "def show_images():\n",
    "    f, ax = plt.subplots(5, 5, figsize=(15,15))\n",
    "    for i in range(len(x_train[:25])):\n",
    "        label = y_train[i]\n",
    "        img = x_train[i]\n",
    "        \n",
    "        if label[0]  == 1: \n",
    "            str_label='Dog'\n",
    "        else:\n",
    "            str_label='Cat'\n",
    "            \n",
    "        ax[i//5, i%5].imshow(img)\n",
    "        ax[i//5, i%5].axis('off')\n",
    "        ax[i//5, i%5].set_title(\"Label: {}\".format(str_label))\n",
    "    plt.show()\n",
    "        \n",
    "show_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Normalization and LabelEncoding\n",
    "Another important thing in Neural networks is normalizing and feature scaling that is bringing all the values in X to a comparable state.\n",
    "<br>\n",
    "Here we will transform each **pixel value to lie between 0 to 1** by dividing each pixel by 255.\n",
    "We have already done label encoding or one hot encoding in this case. If the images is of dog make label for that index as [0,1] else if it is dog make it as [1,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "if nrml == False:\n",
    "    x_train = x_train/255\n",
    "    x_test = x_test/255\n",
    "    nrml = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5176471 , 0.42352942, 0.39215687],\n",
       "        [0.3372549 , 0.32941177, 0.30980393],\n",
       "        [0.32941177, 0.39215687, 0.3764706 ],\n",
       "        ...,\n",
       "        [0.5803922 , 0.5764706 , 0.57254905],\n",
       "        [0.57254905, 0.5647059 , 0.56078434],\n",
       "        [0.5568628 , 0.54901963, 0.54509807]],\n",
       "\n",
       "       [[0.47058824, 0.38039216, 0.34901962],\n",
       "        [0.28627452, 0.27450982, 0.25490198],\n",
       "        [0.3137255 , 0.3764706 , 0.3647059 ],\n",
       "        ...,\n",
       "        [0.6039216 , 0.6       , 0.59607846],\n",
       "        [0.59607846, 0.5882353 , 0.58431375],\n",
       "        [0.58431375, 0.5764706 , 0.57254905]],\n",
       "\n",
       "       [[0.5058824 , 0.41568628, 0.38431373],\n",
       "        [0.32156864, 0.3137255 , 0.29411766],\n",
       "        [0.34509805, 0.4117647 , 0.39607844],\n",
       "        ...,\n",
       "        [0.62352943, 0.6156863 , 0.6117647 ],\n",
       "        [0.6117647 , 0.6039216 , 0.6       ],\n",
       "        [0.59607846, 0.5882353 , 0.58431375]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.49411765, 0.50980395, 0.5254902 ],\n",
       "        [0.50980395, 0.52156866, 0.5411765 ],\n",
       "        [0.5294118 , 0.5411765 , 0.56078434],\n",
       "        ...,\n",
       "        [0.6627451 , 0.6431373 , 0.6313726 ],\n",
       "        [0.654902  , 0.63529414, 0.62352943],\n",
       "        [0.6627451 , 0.6392157 , 0.627451  ]],\n",
       "\n",
       "       [[0.49019608, 0.5019608 , 0.52156866],\n",
       "        [0.50980395, 0.52156866, 0.5411765 ],\n",
       "        [0.5294118 , 0.5411765 , 0.56078434],\n",
       "        ...,\n",
       "        [0.64705884, 0.6313726 , 0.61960787],\n",
       "        [0.6392157 , 0.61960787, 0.60784316],\n",
       "        [0.6313726 , 0.6117647 , 0.6       ]],\n",
       "\n",
       "       [[0.4862745 , 0.49803922, 0.5176471 ],\n",
       "        [0.5019608 , 0.5137255 , 0.53333336],\n",
       "        [0.5176471 , 0.5294118 , 0.54901963],\n",
       "        ...,\n",
       "        [0.6313726 , 0.6117647 , 0.6       ],\n",
       "        [0.61960787, 0.6       , 0.5882353 ],\n",
       "        [0.61960787, 0.6       , 0.5882353 ]]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now check the values of each pixels\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Creating out Model\n",
    "### But before that lets dive into the components of CNN Architecture\n",
    "CNN’s make use of filters (also known as kernels), to detect what features, such as edges, are present throughout an image. A filter is just a matrix of values, called weights, that are trained to detect specific features. The filter moves over each part of the image to check if the feature it is meant to detect is present. To provide a value representing how confident it is that a specific feature is present, the filter carries out a convolution operation, which is an element-wise product and sum between two matrices. \n",
    "\n",
    "\n",
    "**Z = matrix_mutl(W,X) + b**\n",
    "\n",
    "What would be the shape of next layer is governed by this formula - \n",
    "![Formula](assets/formula.png)\n",
    "\n",
    "Where ,\n",
    "n - previous layer size\n",
    "s- strides\n",
    "f- filter size\n",
    "\n",
    "\n",
    "![SegmentLocal](assets/convolve.gif)\n",
    "\n",
    "Other operations or layers involved in Convolution Neural Network architecture are - \n",
    "- **Pooling** - Pooling Layer. It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting.\n",
    "\n",
    "![SegmentLocal](assets/maxpooling.gif)\n",
    "\n",
    "- **Fully Connected Layer** - The output from the convolutional layers represents high-level features in the data. While that output could be flattened and connected to the output layer, adding a fully-connected layer is a (usually) cheap way of learning non-linear combinations of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating out model \n",
    "# We will be using keras to create this neural network model\n",
    "# import keras required  modules\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatdogNet-16\n",
    "A scaled down version of the VGG-16, with a few notable changes.\n",
    "\n",
    "- Number of convolution filters cut in half, fully connected (dense) layers scaled down.\n",
    "- Optimizer changed to RMSprop.\n",
    "- Output layer activation set to sigmoid for binary crossentropy.\n",
    "- Some layers commented out for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'sgd'\n",
    "objective = 'categorical_crossentropy'\n",
    "ROWS = 64\n",
    "COLS = 64\n",
    "\n",
    "\n",
    "def catdog():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(ROWS, COLS, 3), activation='relu'))\n",
    "    model.add(Conv2D(32,(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3),  activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3),  activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "#     model.add(Conv2D(256, (3, 3),  activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_72 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 60, 60, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 26, 26, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 11, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 9, 9, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 2, 2, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 714,274\n",
      "Trainable params: 714,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = catdog()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=1, mode='auto')\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "          validation_split=0.25, verbose=0, shuffle=True, callbacks=[early_stopper])\n",
    "\n",
    "\n",
    "# model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_history(history):\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        # summarize history for loss\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
